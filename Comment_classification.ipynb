{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21726856-4851-4308-b65f-6e4ad50dc3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbe9a9e-08b8-4cb2-a298-191ac1ec0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bddb6c-0527-4274-8867-b5ed83ecc32b",
   "metadata": {},
   "source": [
    "# Text Preprocecssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d543d7b4-2488-4910-92ce-b27d647110a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data with file handlers\n",
    "reviewfile = open('reviews.txt', 'r')\n",
    "reviews = list(map(lambda x:x[:-1], reviewfile.readlines()))\n",
    "\n",
    "labelfile = open('labels.txt', 'r')\n",
    "label = list(map(lambda x:x[:-1], labelfile.readlines()))\n",
    "\n",
    "label = [i.replace('negative\\\\', '0') for i in label]\n",
    "label = [i.replace('positive\\\\', '1') for i in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f8dd55b-b4c7-4793-8dff-d175aa8e1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all of the labels into integer values\n",
    "labels = []\n",
    "for i in label:\n",
    "    integer = int(i)\n",
    "    labels.append(integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48bac162-dead-43f8-9390-07a80482ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing and indicising the data\n",
    "train_data, test_data = reviews[:8000], reviews[8000:11000]\n",
    "train_labels, test_labels = labels[:8000], labels[8000:11000]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_vocabulary(datasets):\n",
    "    for dataset in datasets:\n",
    "        for text in dataset:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocabulary([train_data, test_data]), min_freq=2, specials=[\"<UNK>\"])\n",
    "\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ecf1a58-ff6b-464f-9cee-b70e8c2bf48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32322"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the length of the total vocabulary is\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c92418d-2b78-47fc-8059-2d67825cac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, indicies_list, labels_list):\n",
    "        self.indices = indicies_list\n",
    "        self.labels = labels_list\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.labels[idx], self.indices[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b8cc319-8205-4196-96e0-9adfddd508b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset(train_data, train_labels)\n",
    "test_set = dataset(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f43f497-104c-408d-aadc-58dce2cbeeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = ['positive', 'negative']\n",
    "max_words = 25\n",
    "\n",
    "#zero_padding the indicized sentences and loading them\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [vocab(tokenizer(text)) for text in X]\n",
    "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y) ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]\n",
    "\n",
    "\n",
    "#creating a dataloader with 1024 btch size and also using the collate_fn\n",
    "train_loader = DataLoader(train_set, batch_size=25, collate_fn=vectorize_batch, shuffle=True)\n",
    "test_loader  = DataLoader(test_set , batch_size=25, collate_fn=vectorize_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456f586-c8bb-4709-a599-b70eb28debe7",
   "metadata": {},
   "source": [
    "# RNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74dc0da9-1b7e-4e72-b205-1760033ae621",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 100\n",
    "hidden_dim = 50\n",
    "n_layers=1\n",
    "\n",
    "#developing the model architechture\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
    "        self.rnn = nn.RNN(input_size=embed_len, hidden_size=hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        output, hidden = self.rnn(embeddings, torch.randn(n_layers, len(X_batch), hidden_dim))\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1696c704-0b11-44b1-af02-3d2988057658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embedding_layer): Embedding(32322, 100)\n",
       "  (rnn): RNN(100, 50)\n",
       "  (linear): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_classifier = RNNClassifier()\n",
    "rnn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09b45d6b-97a8-4cde-b829-f5c89d47a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
    "\n",
    "\n",
    "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        for X, Y in tqdm(train_loader):\n",
    "            Y_preds = model(X)\n",
    "\n",
    "            loss = loss_fn(Y_preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        CalcValLossAndAccuracy(model, loss_fn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cdd550e-c9e8-42f5-b74a-bc3bd753fc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 29.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.704\n",
      "Valid Loss : 0.686\n",
      "Valid Acc  : 0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 31.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.690\n",
      "Valid Loss : 0.677\n",
      "Valid Acc  : 0.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 31.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.681\n",
      "Valid Loss : 0.669\n",
      "Valid Acc  : 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:09<00:00, 32.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.671\n",
      "Valid Loss : 0.651\n",
      "Valid Acc  : 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 30.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.658\n",
      "Valid Loss : 0.632\n",
      "Valid Acc  : 0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 30.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.640\n",
      "Valid Loss : 0.613\n",
      "Valid Acc  : 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 30.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.624\n",
      "Valid Loss : 0.597\n",
      "Valid Acc  : 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:09<00:00, 32.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.605\n",
      "Valid Loss : 0.599\n",
      "Valid Acc  : 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:09<00:00, 32.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.591\n",
      "Valid Loss : 0.567\n",
      "Valid Acc  : 0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 30.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.578\n",
      "Valid Loss : 0.545\n",
      "Valid Acc  : 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 29.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.567\n",
      "Valid Loss : 0.548\n",
      "Valid Acc  : 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:09<00:00, 32.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.556\n",
      "Valid Loss : 0.533\n",
      "Valid Acc  : 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:10<00:00, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.550\n",
      "Valid Loss : 0.527\n",
      "Valid Acc  : 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:09<00:00, 33.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.543\n",
      "Valid Loss : 0.509\n",
      "Valid Acc  : 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:11<00:00, 27.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.540\n",
      "Valid Loss : 0.510\n",
      "Valid Acc  : 0.698\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 15\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "rnn_classifier = RNNClassifier()\n",
    "optimizer = Adam(rnn_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "TrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35232cfc-eebd-4c81-9494-25e4798280f8",
   "metadata": {},
   "source": [
    "# LSTM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e0931f5-b58d-4bc6-960c-0b7ae46c4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the LSTM architechture\n",
    "embed_len = 100\n",
    "hidden_dim = 75\n",
    "n_layers = 1\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
    "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
    "        \n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim), torch.randn(n_layers, len(X_batch), hidden_dim)\n",
    "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9afd7f44-7aba-4288-add1-ea0b3abb8e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding_layer): Embedding(32322, 100)\n",
       "  (lstm): LSTM(100, 75, batch_first=True)\n",
       "  (linear): Linear(in_features=75, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMClassifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cda5202-3f5f-4dab-afc1-8cf33cd446ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.690\n",
      "Valid Loss : 0.637\n",
      "Valid Acc  : 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.611\n",
      "Valid Loss : 0.509\n",
      "Valid Acc  : 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 25.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.479\n",
      "Valid Loss : 0.390\n",
      "Valid Acc  : 0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 25.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.359\n",
      "Valid Loss : 0.254\n",
      "Valid Acc  : 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:13<00:00, 23.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.234\n",
      "Valid Loss : 0.138\n",
      "Valid Acc  : 0.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:13<00:00, 23.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.141\n",
      "Valid Loss : 0.072\n",
      "Valid Acc  : 0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.075\n",
      "Valid Loss : 0.049\n",
      "Valid Acc  : 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.044\n",
      "Valid Loss : 0.029\n",
      "Valid Acc  : 0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 25.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.028\n",
      "Valid Loss : 0.010\n",
      "Valid Acc  : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:14<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.013\n",
      "Valid Loss : 0.006\n",
      "Valid Acc  : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:14<00:00, 22.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.012\n",
      "Valid Loss : 0.009\n",
      "Valid Acc  : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:13<00:00, 24.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.009\n",
      "Valid Loss : 0.008\n",
      "Valid Acc  : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:12<00:00, 25.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.019\n",
      "Valid Loss : 0.028\n",
      "Valid Acc  : 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:13<00:00, 24.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.008\n",
      "Valid Loss : 0.002\n",
      "Valid Acc  : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 320/320 [00:14<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.011\n",
      "Valid Loss : 0.009\n",
      "Valid Acc  : 0.998\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 15\n",
    "learning_rate = 1e-3 \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = LSTMClassifier()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "TrainModel(model, loss_fn, optimizer, train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06a573-f367-42da-8a8e-a8e7e1f99026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
